{
  "metadata": {
    "name": "CVM",
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%pyspark\nimport requests, zipfile, io, pandas as pd, urllib, threading, time\nfrom datetime import date\nfrom urllib.parse import urljoin\nimport pandas as pd\nfrom pyspark.sql.types import *\nfrom pyspark.sql.functions import *"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%pyspark\ndf_teste \u003d None\ntable_list \u003d []\nnumber_request_done \u003d []\nrequests_list \u003d []\nthreads_list \u003d []"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%pyspark\r\n#funcionou\r\nimport pandas as pd\r\nimport numpy as np\r\nimport requests,threading,time,urllib,requests\r\nimport io\r\nfrom datetime import datetime, timedelta, date\r\nfrom pyspark.sql.functions import col\r\n\r\n\r\ndef get_last_day_previus_month(current_month_date):\r\n    \"\"\"\r\n    Method that return the lest day of previus month \r\n    \"\"\"\r\n    first_day_current_month \u003d current_month_date.replace(day\u003d1)\r\n    previus_month \u003d first_day_current_month - timedelta(days\u003d1)\r\n    return previus_month\r\n\r\ndef make_urls(url,date\u003d date.today(),extencion\u003dNone):\r\n    \"\"\"\r\n    Method that make urls to use in requests\r\n    return is a list with urls with done contain dates \r\n    (current_date until 3 years before)\r\n    But can be used at any string that the client\r\n    whant apply the same strategy   \r\n    \"\"\"\r\n    \r\n    urls \u003d []\r\n    current_date \u003d date\r\n    aux_current_date \u003d current_date\r\n    \r\n    while int(aux_current_date.strftime(\"%Y\")) \u003e (int(current_date.strftime(\"%Y\")) - 4):\r\n\r\n        urls.append(url+aux_current_date.strftime(\"%Y%m\")+\".\"+extencion)\r\n        aux_current_date \u003d get_last_day_previus_month(aux_current_date)\r\n        \r\n    return urls\r\n    \r\n\r\ndef make_request(url):\r\n    \"\"\"\r\n    Method that make request\r\n    Return is the content of request\r\n    \"\"\"\r\n    global requests_list\r\n    print(\"Request to {}\".format(url))\r\n    try:\r\n        \r\n        read_table \u003d requests.get(str(url)).content\r\n        name \u003d url.split(\u0027/\u0027)\r\n        name \u003d name[len(name) - 1].split(\u0027.\u0027)[0]\r\n        aux_dict \u003d {\u0027data\u0027:read_table,\u0027name\u0027:name}\r\n        requests_list.append(aux_dict)\r\n        print(\"Request to {} in list\".format(url))\r\n\r\n    except Exception as e:\r\n        print(\"We had the exception {exception} in {url}\".format(exception\u003dstr(e),url\u003dstr(url)))\r\n        \r\n\r\n\r\n\r\ndef read_tables_from_raw(path_file\u003dNone):\r\n    global df_teste\r\n    from pyspark.sql.types import DateType\r\n    \"\"\"\r\n    This method read and format table to input in ref s3\r\n    \r\n    \"\"\"\r\n\r\n    print(\"\\nReadding path {}\".format(path_file))\r\n    read_table \u003d sqlContext.read.csv(path_file,sep\u003d\";\",header\u003dTrue)\r\n    print(read_table.printSchema())\r\n    df_teste \u003d read_table\r\n    columns_to_decimal \u003d \u0027VL_TOTAL\u0027, \u0027VL_QUOTA\u0027, \u0027VL_PATRIM_LIQ\u0027, \u0027CAPTC_DIA\u0027, \u0027RESG_DIA\u0027\r\n    columns_to_date \u003d [\u0027DT_COMPTC\u0027]\r\n    \r\n    #read_table \u003d read_table.withColumn(\u0027CNPJ_FUNDO\u0027, regexp_replace(read_table[\u0027CNPJ_FUNDO\u0027],\"/\",\"\"))\r\n    #read_table \u003d read_table.withColumn(\u0027CNPJ_FUNDO\u0027, col(regexp_replace(read_table[\u0027CNPJ_FUNDO\u0027],\".\",\"\")))\r\n    for column in columns_to_decimal:\r\n        if column \u003d\u003d \"VL_QUOTA\":\r\n            read_table \u003d read_table.withColumn(column, read_table[column].cast(DecimalType(38,12)))\r\n        \r\n        else:\r\n            read_table \u003d read_table.withColumn(column, read_table[column].cast(DecimalType(38,2)))\r\n    \r\n    read_table \u003d read_table.withColumn(\"NR_COTST\", read_table[\"NR_COTST\"].cast(IntegerType()))\r\n        \r\n    for column in columns_to_date:\r\n        read_table \u003d read_table.withColumn(column, read_table[column].cast(DateType()))\r\n        \r\n        \r\n    name_aux \u003d path_file.split(\"/\")\r\n    name \u003d name_aux[len(name_aux) - 1].split(\u0027.\u0027)[0]\r\n\r\n    aux_dict \u003d {\u0027data\u0027:read_table,\u0027name\u0027:name}\r\n    table_list.append(aux_dict)\r\n    print(table_list)\r\n\r\n\r\n\r\n\r\ndef start_requests(urls):\r\n    global threads_list\r\n    \"\"\" Method that make async requests\"\"\"\r\n    \r\n    for url in urls:\r\n        t \u003d threading.Thread(target\u003dmake_request,args\u003d(str(url),))\r\n        threads_list.append(t)\r\n        t.start()\r\n\r\n    for i in threads_list:\r\n        i.join()\r\n\r\n\r\ndef insert_in_raw_s3(table\u003dNone, date_file\u003dstr(date.today())):\r\n    import boto3\r\n    \r\n\r\n    name \u003d table[\u0027name\u0027]\r\n    data \u003d table[\u0027data\u0027]\r\n    key \u003d \u0027data/raw/external_cvm/run\u003d{date}/{index}.csv\u0027.format(date\u003ddate_file,index\u003dname)\r\n    bucket\u003d\u0027warrenbrasil-datalake\u0027\r\n    s3 \u003d boto3.client(\u0027s3\u0027)\r\n    s3.put_object(Body\u003ddata, Bucket\u003dbucket, Key\u003dkey)\r\n    print(\"Tabela salva no raw s3 path: {}\".format(key))\r\n\r\n\r\ndef insert_in_s3(table):\r\n    \r\n    name \u003d table[\u0027name\u0027]\r\n    data \u003d table[\u0027data\u0027]\r\n    aux_date \u003d date.today()\r\n    try:\r\n\r\n        #data.write.csv(\"s3://warrenbrasil-homlogacao-dl/temp/CVM/ref/run\u003d{}/{}\".format(aux_date,name),mode\u003d\"overwrite\")\r\n        data.write.format(\"parquet\").mode(\"overwrite\").save(\"s3://warrenbrasil-datalake/data/ref/external_cvm/run\u003d{}/{}\".format(aux_date,name))\r\n        print(\"Table {} in s3\".format(name))\r\n        #print(data.show(100))\r\n    except Exception as e:\r\n        print(e)\r\n        \r\n   "
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%pyspark\n\n# ~3min\nstart \u003d datetime.today()\n\nurls \u003d make_urls(url\u003dr\"http://dados.cvm.gov.br/dados/FI/DOC/INF_DIARIO/DADOS/inf_diario_fi_\",extencion\u003d\"csv\")\nstart_requests(urls)\n\n\nfor i in requests_list:\n    insert_in_raw_s3(table\u003di)\n\nend \u003d datetime.today()\nprint(\"finalizou\")\nprint(\"Time was start:{} and end:{}\".format(str(start),str(end)))\n"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%pyspark\npath_file \u003d \"s3://warrenbrasil-datalake/data/raw/external_cvm/run\u003d{}/inf_diario_fi_\".format(date.today())\npath_tables_of_raw \u003d  make_urls(path_file,date\u003d date.today(),extencion\u003d\"csv\")"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%pyspark\nfor i in path_tables_of_raw:\n    print(i)"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%pyspark\nimport pandas as pd\nimport numpy as np\nimport requests,threading,time,urllib,requests\nimport io\nfrom datetime import datetime, timedelta, date\n\nstart \u003d datetime.today()\ntheads_list \u003d []\nfor i in path_tables_of_raw:\n    t_B \u003d threading.Thread(target\u003dread_tables_from_raw, args\u003d(i,))\n    theads_list.append(t_B)\n    t_B.start()\n\nfor i in theads_list:\n    i.join()\n\n"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%pyspark\n\n# ~2min\nprint(len(table_list))\n\nfor i in table_list:\n    try:\n        teste \u003d i[\"data\"]\n        teste.createOrReplaceTempView(\"teste\")\n        insert_in_s3(i)\n    except SparkException as e:\n        print(\" Algum problema em {}\".format(i[\"name\"]))\n    \n\nend \u003d datetime.today()\nprint(\"finalizou\")\nprint(\"Time was start:{} and end:{}\".format(str(start),str(end)))"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%pyspark\n"
    }
  ]
}